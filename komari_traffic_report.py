#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import logging
import os
import re
import sys
import time
import traceback
import socket
import gzip
import concurrent.futures
import signal
from dataclasses import dataclass
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo

import requests
import random
from requests.adapters import HTTPAdapter
from urllib3.util import Retry

STAT_TZ = os.environ.get("STAT_TZ", "Asia/Shanghai")
TZ = ZoneInfo(STAT_TZ)  # ç»Ÿè®¡æ—¶åŒº

KOMARI_BASE_URL = os.environ.get("KOMARI_BASE_URL", "").rstrip("/")
TELEGRAM_BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN", "")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID", "")
DATA_DIR = os.environ.get("DATA_DIR", "/var/lib/komari-traffic")

HISTORY_HOT_DAYS = int(os.environ.get("HISTORY_HOT_DAYS", "60"))
HISTORY_RETENTION_DAYS = int(os.environ.get("HISTORY_RETENTION_DAYS", "400"))

KOMARI_API_TOKEN = os.environ.get("KOMARI_API_TOKEN", "")
KOMARI_API_TOKEN_HEADER = os.environ.get("KOMARI_API_TOKEN_HEADER", "Authorization")
KOMARI_API_TOKEN_PREFIX = os.environ.get("KOMARI_API_TOKEN_PREFIX", "Bearer")
KOMARI_FETCH_WORKERS = int(os.environ.get("KOMARI_FETCH_WORKERS", "6"))

TOP_N = int(os.environ.get("TOP_N", "3"))  # é»˜è®¤ Top3ï¼ˆæ—¥æŠ¥/å‘¨æŠ¥/æœˆæŠ¥/Topå‘½ä»¤éƒ½ç”¨å®ƒï¼‰

# /top Nh ä¾èµ–é‡‡æ ·å¿«ç…§ï¼šbot è¿è¡Œæ—¶è‡ªåŠ¨é‡‡æ ·
SAMPLE_INTERVAL_SECONDS = int(os.environ.get("SAMPLE_INTERVAL_SECONDS", "300"))  # é»˜è®¤ 5 åˆ†é’Ÿ
SAMPLE_RETENTION_HOURS = int(os.environ.get("SAMPLE_RETENTION_HOURS", "720"))    # é»˜è®¤ä¿ç•™ 30 å¤©é‡‡æ ·

BASELINES_PATH = os.path.join(DATA_DIR, "baselines.json")
HISTORY_PATH = os.path.join(DATA_DIR, "history.json")
SAMPLES_PATH = os.path.join(DATA_DIR, "samples.json")
TG_OFFSET_PATH = os.path.join(DATA_DIR, "tg_offset.txt")

TIMEOUT = int(os.environ.get("KOMARI_TIMEOUT_SECONDS", "15"))  # Komari API timeoutï¼ˆç§’ï¼‰

LOG_LEVEL = os.environ.get("LOG_LEVEL", "INFO").upper()
LOG_FILE = os.environ.get("LOG_FILE", "").strip()

SHUTTING_DOWN = False


def build_http_session() -> requests.Session:
    retry = Retry(
        total=3,
        connect=3,
        read=3,
        status=3,
        backoff_factor=0.5,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=frozenset(["GET"]),
    )
    adapter = HTTPAdapter(max_retries=retry)
    session = requests.Session()
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session


HTTP_SESSION = build_http_session()


def setup_logging():
    handlers: list[logging.Handler] = [logging.StreamHandler()]
    if LOG_FILE:
        handlers.append(logging.FileHandler(LOG_FILE, encoding="utf-8"))
    logging.basicConfig(
        level=getattr(logging, LOG_LEVEL, logging.INFO),
        format="%(asctime)s %(levelname)s %(message)s",
        handlers=handlers,
    )


def build_komari_headers() -> dict:
    headers = {"Accept": "application/json"}
    if KOMARI_API_TOKEN:
        prefix = KOMARI_API_TOKEN_PREFIX.strip()
        value = f"{prefix} {KOMARI_API_TOKEN}".strip()
        headers[KOMARI_API_TOKEN_HEADER] = value
    return headers


def _require_positive_int(name: str, value: int):
    if value <= 0:
        raise RuntimeError(f"{name} must be > 0")


def validate_config_or_raise():
    required = {
        "KOMARI_BASE_URL": KOMARI_BASE_URL,
        "TELEGRAM_BOT_TOKEN": TELEGRAM_BOT_TOKEN,
        "TELEGRAM_CHAT_ID": TELEGRAM_CHAT_ID,
    }
    missing = [k for k, v in required.items() if not str(v).strip()]
    if missing:
        raise RuntimeError("Missing required env: " + ", ".join(missing))

    _require_positive_int("KOMARI_TIMEOUT_SECONDS", TIMEOUT)
    _require_positive_int("KOMARI_FETCH_WORKERS", KOMARI_FETCH_WORKERS)
    _require_positive_int("TOP_N", TOP_N)
    _require_positive_int("SAMPLE_INTERVAL_SECONDS", SAMPLE_INTERVAL_SECONDS)
    _require_positive_int("SAMPLE_RETENTION_HOURS", SAMPLE_RETENTION_HOURS)


def run_healthcheck_or_raise():
    ensure_dirs()

    test_path = os.path.join(DATA_DIR, ".health_write_test")
    try:
        with open(test_path, "w", encoding="utf-8") as f:
            f.write("ok")
        os.remove(test_path)
    except Exception as e:
        raise RuntimeError(f"DATA_DIR not writable: {DATA_DIR}: {e}")

    for p in [BASELINES_PATH, HISTORY_PATH, SAMPLES_PATH, TG_OFFSET_PATH]:
        if os.path.exists(p):
            try:
                if p == TG_OFFSET_PATH:
                    _ = load_offset()
                else:
                    load_json(p, {})
            except Exception as e:
                raise RuntimeError(f"Corrupted file: {p}: {e}")

    try:
        HTTP_SESSION.get(KOMARI_BASE_URL, timeout=TIMEOUT, headers=build_komari_headers())
    except Exception as e:
        raise RuntimeError(f"Komari unreachable: {e}")


def _handle_sigterm(signum, _frame):
    global SHUTTING_DOWN
    SHUTTING_DOWN = True
    logging.warning("received signal %s, shutting down gracefully...", signum)


# -------------------- åŸºç¡€å·¥å…· --------------------

def ensure_dirs():
    os.makedirs(DATA_DIR, exist_ok=True)
    try:
        os.chmod(DATA_DIR, 0o700)
    except Exception:
        pass


def human_bytes(n: int) -> str:
    units = ["B", "KiB", "MiB", "GiB", "TiB", "PiB"]
    x = float(max(int(n), 0))
    for u in units:
        if x < 1024 or u == units[-1]:
            return f"{x:.2f} {u}" if u != "B" else f"{int(x)} B"
        x /= 1024
    return f"{x:.2f} PiB"


def now_dt() -> datetime:
    return datetime.now(TZ)


def today_date() -> date:
    return now_dt().date()


def start_of_day(d: date) -> datetime:
    return datetime(d.year, d.month, d.day, 0, 0, 0, tzinfo=TZ)


def start_of_week(d: date) -> date:
    return d - timedelta(days=d.weekday())  # å‘¨ä¸€ä¸ºèµ·ç‚¹


def start_of_month(d: date) -> date:
    return date(d.year, d.month, 1)


def yyyymm(d: date) -> str:
    return f"{d.year:04d}-{d.month:02d}"


def load_json(path: str, default):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return default
    except Exception:
        return default


def save_json_atomic(path: str, data):
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    os.replace(tmp, path)


def get_json(url: str):
    r = HTTP_SESSION.get(url, timeout=TIMEOUT, headers=build_komari_headers())
    r.raise_for_status()
    return r.json()


def post_json(url: str, payload: dict):
    r = requests.post(url, json=payload, timeout=TIMEOUT)
    r.raise_for_status()
    return r.json()


def telegram_send(text: str):
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        raise RuntimeError("TELEGRAM_BOT_TOKEN / TELEGRAM_CHAT_ID æœªè®¾ç½®")
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": True,
    }
    return post_json(url, payload)


def safe_telegram_send(text: str):
    try:
        if TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID:
            telegram_send(text)
    except Exception:
        pass


def should_alert(throttle_key: str, min_interval_seconds: int = 300) -> bool:
    ensure_dirs()
    state_path = os.path.join(DATA_DIR, f"alert_{throttle_key}.json")
    now_ts = int(time.time())
    state = load_json(state_path, {"last": 0})
    last = int(state.get("last", 0))
    if now_ts - last < min_interval_seconds:
        return False
    save_json_atomic(state_path, {"last": now_ts})
    return True


def alert_exception(where: str, cmd: str, exc: Exception):
    host = socket.gethostname()
    ts = now_dt().strftime("%Y-%m-%d %H:%M:%S %Z")
    err = f"{type(exc).__name__}: {exc}"
    tb = traceback.format_exc()
    tb_tail = (tb[-1500:]).replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
    msg = (
        f"âŒ <b>Komari æµé‡ä»»åŠ¡å¤±è´¥</b>\n"
        f"ğŸ•’ {ts}\n"
        f"ğŸ–¥ {host}\n"
        f"ğŸ“ {where}\n"
        f"ğŸ§© cmd: <code>{cmd}</code>\n"
        f"ğŸ§¨ error: <code>{err}</code>\n\n"
        f"<b>traceback (tail)</b>\n<pre>{tb_tail}</pre>"
    )
    safe_telegram_send(msg)


# -------------------- Komari æ•°æ®è·å–ï¼ˆç¨³æ€ï¼šå•èŠ‚ç‚¹è¶…æ—¶/å¼‚å¸¸è·³è¿‡ï¼‰ --------------------

@dataclass
class NodeTotal:
    uuid: str
    name: str
    up: int
    down: int


def fetch_nodes_and_totals():
    """
    è¿”å›ï¼š
      - out: list[NodeTotal]
      - skipped: list[str]  # è¢«è·³è¿‡çš„èŠ‚ç‚¹åŸå› ï¼ˆtimeout/empty/bad_resp/HTTPErrorç­‰ï¼‰
    """
    if not KOMARI_BASE_URL:
        raise RuntimeError("KOMARI_BASE_URL æœªè®¾ç½®ï¼ˆä¾‹å¦‚ https://komari.exampleï¼‰")

    nodes_resp = get_json(f"{KOMARI_BASE_URL}/api/nodes")
    if not (isinstance(nodes_resp, dict) and nodes_resp.get("status") == "success"):
        raise RuntimeError(f"/api/nodes è¿”å›å¼‚å¸¸ï¼š{nodes_resp}")

    nodes = nodes_resp.get("data", [])
    out: list[NodeTotal] = []
    skipped: list[str] = []

    def fetch_one(node: dict):
        uuid = node.get("uuid")
        name = node.get("name") or uuid
        if not uuid:
            return None, None
        try:
            recent_resp = get_json(f"{KOMARI_BASE_URL}/api/recent/{uuid}")
        except requests.exceptions.ReadTimeout:
            return None, f"{name}(timeout)"
        except requests.exceptions.RequestException as e:
            return None, f"{name}({type(e).__name__})"
        except Exception as e:
            return None, f"{name}({type(e).__name__})"

        if not (isinstance(recent_resp, dict) and recent_resp.get("status") == "success"):
            return None, f"{name}(bad_resp)"

        points = recent_resp.get("data", [])
        if not points:
            return None, f"{name}(empty)"

        last = points[-1]
        net = last.get("network", {}) if isinstance(last, dict) else {}
        up = int(net.get("totalUp", 0))
        down = int(net.get("totalDown", 0))
        return NodeTotal(uuid=uuid, name=name, up=up, down=down), None

    max_workers = max(1, min(len(nodes), KOMARI_FETCH_WORKERS))
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_map = {executor.submit(fetch_one, n): n for n in nodes}
        for future in concurrent.futures.as_completed(future_map):
            result, skip = future.result()
            if skip:
                skipped.append(skip)
                continue
            if result:
                out.append(result)

    return out, skipped


def build_nodes_map_from_current(current: list[NodeTotal]) -> dict:
    return {n.uuid: {"name": n.name, "up": n.up, "down": n.down} for n in current}


def compute_delta_from_nodes(current: list[NodeTotal], baseline_nodes: dict) -> tuple[dict, dict, list[str]]:
    """
    baseline_nodes: {uuid:{name,up,down}}
    returns: (deltas, new_baseline_nodes, reset_warnings)
    """
    deltas = {}
    new_baseline = {}
    reset_warnings = []

    for n in current:
        prev = baseline_nodes.get(n.uuid, {})
        prev_up = int(prev.get("up", 0))
        prev_down = int(prev.get("down", 0))

        up_delta = n.up - prev_up
        down_delta = n.down - prev_down

        reset = False
        if up_delta < 0:
            up_delta = n.up
            reset = True
        if down_delta < 0:
            down_delta = n.down
            reset = True
        if reset:
            reset_warnings.append(n.name)

        deltas[n.uuid] = {"name": n.name, "up": up_delta, "down": down_delta}
        new_baseline[n.uuid] = {"name": n.name, "up": n.up, "down": n.down}

    return deltas, new_baseline, reset_warnings


def compute_delta_from_maps(current_nodes_map: dict, baseline_nodes_map: dict) -> tuple[dict, list[str]]:
    """
    current_nodes_map / baseline_nodes_map:
      {uuid:{name,up,down}}
    returns: (deltas, reset_warnings)
    """
    deltas = {}
    reset_warnings = []
    for uuid, cur in current_nodes_map.items():
        prev = baseline_nodes_map.get(uuid, {})
        name = cur.get("name", uuid)

        cur_up = int(cur.get("up", 0))
        cur_down = int(cur.get("down", 0))
        prev_up = int(prev.get("up", 0))
        prev_down = int(prev.get("down", 0))

        up_delta = cur_up - prev_up
        down_delta = cur_down - prev_down

        reset = False
        if up_delta < 0:
            up_delta = cur_up
            reset = True
        if down_delta < 0:
            down_delta = cur_down
            reset = True
        if reset:
            reset_warnings.append(name)

        deltas[uuid] = {"name": name, "up": up_delta, "down": down_delta}

    return deltas, reset_warnings


# -------------------- Top æ¦œå±•ç¤º --------------------

def top_lines(deltas: dict, n: int) -> list[str]:
    items = []
    for v in deltas.values():
        name = v.get("name", "")
        up = int(v.get("up", 0))
        down = int(v.get("down", 0))
        total = up + down
        items.append((total, down, up, name))

    items.sort(reverse=True, key=lambda x: (x[0], x[1], x[2], x[3].lower()))
    top = items[: max(0, int(n))]

    if not top:
        return ["ï¼ˆæš‚æ— æ•°æ®ï¼‰"]

    rows = []
    for i, (total, down, up, name) in enumerate(top, start=1):
        rows.append(
            f"{i}ï¸âƒ£ <b>{name}</b>ï¼š{human_bytes(total)}"
            f"ï¼ˆâ¬‡ï¸ {human_bytes(down)} / â¬†ï¸ {human_bytes(up)}ï¼‰"
        )
    return rows


def format_report(title: str, period_label: str, deltas: dict, reset_warnings: list[str], skipped: list[str] | None = None, include_top: bool = True) -> str:
    skipped = skipped or []

    lines = [f"ğŸ“Š <b>{title}</b>ï¼ˆ{period_label}ï¼‰", ""]
    total_up = 0
    total_down = 0

    items = sorted(deltas.values(), key=lambda x: (x.get("name") or "").lower())
    for it in items:
        total_up += int(it["up"])
        total_down += int(it["down"])
        lines.append(
            f"ğŸ–¥ <b>{it['name']}</b>\n"
            f"â¬‡ï¸ ä¸‹è¡Œï¼š{human_bytes(it['down'])}\n"
            f"â¬†ï¸ ä¸Šè¡Œï¼š{human_bytes(it['up'])}\n"
        )

    lines.append("â€”â€”")
    lines.append(f"ğŸ“¦ <b>æ€»ä¸‹è¡Œ</b>ï¼š{human_bytes(total_down)}")
    lines.append(f"ğŸ“¦ <b>æ€»ä¸Šè¡Œ</b>ï¼š{human_bytes(total_up)}")
    lines.append(f"ğŸ“¦ <b>æ€»åˆè®¡</b>ï¼š{human_bytes(total_down + total_up)}")

    if include_top:
        lines.append("")
        lines.append(f"ğŸ”¥ <b>Top {TOP_N} æ¶ˆè€—æ¦œ</b>ï¼ˆä¸Šä¸‹è¡Œåˆè®¡ï¼‰")
        lines.extend(top_lines(deltas, n=TOP_N))

    if skipped:
        lines.append("")
        lines.append("âš ï¸ <b>ä»¥ä¸‹èŠ‚ç‚¹å› å¼‚å¸¸è¢«è·³è¿‡</b>ï¼š")
        lines.append("ã€".join(skipped[:30]) + ("â€¦â€¦" if len(skipped) > 30 else ""))

    if reset_warnings:
        lines.append("")
        lines.append("âš ï¸ <b>æ£€æµ‹åˆ°è®¡æ•°å™¨å¯èƒ½é‡ç½®</b>ï¼ˆå·²å…œåº•ï¼‰ï¼š")
        lines.append("ã€".join(reset_warnings))

    return "\n".join(lines)


def send_top_only(period_label: str, deltas: dict, reset_warnings: list[str], skipped: list[str] | None = None):
    skipped = skipped or []
    lines = [f"ğŸ”¥ <b>Top {TOP_N} æ¶ˆè€—æ¦œ</b>ï¼ˆä¸Šä¸‹è¡Œåˆè®¡ï¼‰", f"â± {period_label}", ""]
    lines.extend(top_lines(deltas, n=TOP_N))

    if skipped:
        lines.append("")
        lines.append("âš ï¸ <b>ä»¥ä¸‹èŠ‚ç‚¹å› å¼‚å¸¸è¢«è·³è¿‡</b>ï¼š")
        lines.append("ã€".join(skipped[:30]) + ("â€¦â€¦" if len(skipped) > 30 else ""))

    if reset_warnings:
        lines.append("")
        lines.append("âš ï¸ <b>æ£€æµ‹åˆ°è®¡æ•°å™¨å¯èƒ½é‡ç½®</b>ï¼ˆå·²å…œåº•ï¼‰ï¼š")
        lines.append("ã€".join(reset_warnings))

    telegram_send("\n".join(lines))


# -------------------- å†å²æ•°æ®ï¼šçƒ­å­˜å‚¨ + å†·å½’æ¡£ï¼ˆgzipï¼‰ --------------------

def archive_path_for_month(ym: str) -> str:
    return os.path.join(DATA_DIR, f"history-{ym}.json.gz")


def load_archive_month(ym: str) -> dict:
    path = archive_path_for_month(ym)
    if not os.path.exists(path):
        return {"days": {}}
    with gzip.open(path, "rt", encoding="utf-8") as f:
        return json.load(f)


def save_archive_month(ym: str, data: dict):
    path = archive_path_for_month(ym)
    tmp = path + ".tmp"
    with gzip.open(tmp, "wt", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False)
    os.replace(tmp, path)


def history_append(day_str: str, deltas: dict):
    hist = load_json(HISTORY_PATH, {"days": {}})
    hist.setdefault("days", {})
    hist["days"][day_str] = deltas
    save_json_atomic(HISTORY_PATH, hist)


def archive_and_prune_history():
    ensure_dirs()
    hist = load_json(HISTORY_PATH, {"days": {}})
    days: dict = hist.get("days", {})
    if not days:
        return

    today = today_date()
    hot_cut = today - timedelta(days=HISTORY_HOT_DAYS)
    retention_cut = today - timedelta(days=HISTORY_RETENTION_DAYS)

    to_keep = {}
    to_archive_by_month: dict[str, dict] = {}

    for k, v in days.items():
        try:
            d = datetime.strptime(k, "%Y-%m-%d").date()
        except Exception:
            continue

        if d < retention_cut:
            continue

        if d < hot_cut:
            ym = yyyymm(d)
            to_archive_by_month.setdefault(ym, {})
            to_archive_by_month[ym][k] = v
        else:
            to_keep[k] = v

    for ym, month_days in to_archive_by_month.items():
        arc = load_archive_month(ym)
        arc.setdefault("days", {})
        arc["days"].update(month_days)

        pruned = {}
        for dk, dv in arc["days"].items():
            try:
                dd = datetime.strptime(dk, "%Y-%m-%d").date()
            except Exception:
                continue
            if dd >= retention_cut:
                pruned[dk] = dv
        arc["days"] = pruned
        save_archive_month(ym, arc)

    save_json_atomic(HISTORY_PATH, {"days": to_keep})


def history_sum(from_day: date, to_day: date) -> dict:
    ensure_dirs()
    summed = {}
    hot = load_json(HISTORY_PATH, {"days": {}}).get("days", {})

    def add_one_day(one: dict):
        for uuid, v in one.items():
            if uuid not in summed:
                summed[uuid] = {"name": v.get("name", uuid), "up": 0, "down": 0}
            summed[uuid]["up"] += int(v.get("up", 0))
            summed[uuid]["down"] += int(v.get("down", 0))

    d = from_day
    while d <= to_day:
        key = d.strftime("%Y-%m-%d")
        if key in hot:
            add_one_day(hot.get(key, {}))
        else:
            ym = yyyymm(d)
            arc = load_archive_month(ym).get("days", {})
            add_one_day(arc.get(key, {}))
        d += timedelta(days=1)

    return summed


# -------------------- Baselineï¼ˆæŒ‰ tagï¼‰ --------------------

def load_baselines():
    return load_json(BASELINES_PATH, {"baselines": {}})


def save_baseline(tag: str, nodes: dict):
    base = load_baselines()
    base.setdefault("baselines", {})
    base["baselines"][tag] = {
        "nodes": nodes,
        "ts": now_dt().strftime("%Y-%m-%d %H:%M:%S %Z"),
    }
    save_json_atomic(BASELINES_PATH, base)


def get_baseline_nodes(tag: str) -> dict | None:
    base = load_baselines()
    b = base.get("baselines", {}).get(tag)
    if not b:
        return None
    return b.get("nodes", {})


def set_baseline_to_current(tag: str):
    ensure_dirs()
    current, _skipped = fetch_nodes_and_totals()
    save_baseline(tag, build_nodes_map_from_current(current))


# -------------------- é‡‡æ ·å™¨ï¼ˆç”¨äº /top Nhï¼‰ --------------------

def load_samples():
    return load_json(SAMPLES_PATH, {"samples": []})


def save_samples(data: dict):
    save_json_atomic(SAMPLES_PATH, data)


def prune_samples(samples: list, now_ts: int):
    keep_after = now_ts - SAMPLE_RETENTION_HOURS * 3600
    pruned = [s for s in samples if int(s.get("ts", 0)) >= keep_after]
    pruned.sort(key=lambda x: int(x.get("ts", 0)))
    return pruned


def take_sample_if_due(force: bool = False):
    """
    ç”± bot å¾ªç¯å‘¨æœŸæ€§è°ƒç”¨ï¼šæœ€å¤šæ¯ SAMPLE_INTERVAL_SECONDS é‡‡æ ·ä¸€æ¬¡
    """
    ensure_dirs()
    data = load_samples()
    samples = data.get("samples", [])
    now_ts = int(time.time())

    last_ts = int(samples[-1]["ts"]) if samples else 0
    if (not force) and last_ts and (now_ts - last_ts < SAMPLE_INTERVAL_SECONDS):
        return

    current, skipped = fetch_nodes_and_totals()
    nodes_map = build_nodes_map_from_current(current)

    samples.append({"ts": now_ts, "nodes": nodes_map, "skipped": skipped})
    samples = prune_samples(samples, now_ts)
    save_samples({"samples": samples})


def get_sample_at_or_before(target_ts: int):
    data = load_samples()
    samples = data.get("samples", [])
    if not samples:
        return None

    lo, hi = 0, len(samples) - 1
    best = None
    while lo <= hi:
        mid = (lo + hi) // 2
        ts = int(samples[mid].get("ts", 0))
        if ts <= target_ts:
            best = samples[mid]
            lo = mid + 1
        else:
            hi = mid - 1
    return best


# -------------------- æŠ¥è¡¨ä»»åŠ¡ --------------------

def run_daily_send_yesterday():
    """
    æ¯å¤© 00:00ï¼šå‘é€æ˜¨æ—¥æ—¥æŠ¥ï¼›å†™å…¥ historyï¼›å½’æ¡£ï¼›å¹¶å†™å…¥â€œä»Šæ—¥èµ·ç‚¹ baseline(YYYY-MM-DD)â€
    """
    ensure_dirs()
    yday = today_date() - timedelta(days=1)
    yday_label = yday.strftime("%Y-%m-%d")

    baseline_nodes = get_baseline_nodes(yday_label)
    current, skipped = fetch_nodes_and_totals()

    if baseline_nodes is None:
        save_baseline(yday_label, build_nodes_map_from_current(current))
        telegram_send(
            f"âš ï¸ <b>æ—¥æŠ¥åŸºçº¿ç¼ºå¤±</b>ï¼ˆ{yday_label}ï¼‰ã€‚\n"
            f"æˆ‘å·²æŠŠå½“å‰ç´¯è®¡ä¿å­˜ä¸ºè¯¥æ—¥åŸºçº¿ã€‚\n"
            f"ä»ä¸‹ä¸€æ¬¡ 00:00 å¼€å§‹æ—¥æŠ¥å°†ç¨³å®šæ­£å¸¸ã€‚"
        )
        return

    deltas, new_baseline, reset_warnings = compute_delta_from_nodes(current, baseline_nodes)
    telegram_send(format_report("æ˜¨æ—¥æµé‡æ—¥æŠ¥", yday_label, deltas, reset_warnings, skipped=skipped, include_top=True))

    history_append(yday_label, deltas)
    archive_and_prune_history()

    today_label = today_date().strftime("%Y-%m-%d")
    save_baseline(today_label, new_baseline)


def run_weekly_send_last_week():
    ensure_dirs()
    today = today_date()
    this_week_start = start_of_week(today)
    last_week_end = this_week_start - timedelta(days=1)
    last_week_start = last_week_end - timedelta(days=6)

    summed = history_sum(last_week_start, last_week_end)
    label = f"{last_week_start.strftime('%Y-%m-%d')} â†’ {last_week_end.strftime('%Y-%m-%d')}"
    telegram_send(format_report("ä¸Šå‘¨æµé‡å‘¨æŠ¥", label, summed, [], skipped=[], include_top=True))


def run_monthly_send_last_month():
    ensure_dirs()
    today = today_date()
    this_month_start = start_of_month(today)
    last_month_end = this_month_start - timedelta(days=1)
    last_month_start = date(last_month_end.year, last_month_end.month, 1)

    summed = history_sum(last_month_start, last_month_end)
    label = f"{last_month_start.strftime('%Y-%m-%d')} â†’ {last_month_end.strftime('%Y-%m-%d')}"
    telegram_send(format_report("ä¸Šæœˆæµé‡æœˆæŠ¥", label, summed, [], skipped=[], include_top=True))


def run_period_report(from_dt: datetime, to_dt: datetime, tag: str, top_only: bool = False):
    ensure_dirs()
    baseline_nodes = get_baseline_nodes(tag)
    if baseline_nodes is None:
        set_baseline_to_current(tag)
        telegram_send(
            f"âš ï¸ å½“å‰æ²¡æœ‰æ‰¾åˆ° èµ·ç‚¹å¿«ç…§ï¼ˆ{tag}ï¼‰ã€‚\n"
            f"æˆ‘å·²æŠŠç°åœ¨çš„ç´¯è®¡å€¼ä¿å­˜ä¸ºæ–°çš„èµ·ç‚¹ã€‚\n"
            f"è¯·ç¨åå†å‘ä¸€æ¬¡å‘½ä»¤æŸ¥çœ‹ç¨³å®šç»Ÿè®¡ã€‚"
        )
        return

    current, skipped = fetch_nodes_and_totals()
    deltas, _new_base, reset_warnings = compute_delta_from_nodes(current, baseline_nodes)
    period_label = f"{from_dt.strftime('%Y-%m-%d %H:%M')} â†’ {to_dt.strftime('%Y-%m-%d %H:%M')}"

    if top_only:
        send_top_only(period_label, deltas, reset_warnings, skipped=skipped)
    else:
        telegram_send(format_report("æµé‡ç»Ÿè®¡", period_label, deltas, reset_warnings, skipped=skipped, include_top=True))


def run_top_last_hours(hours: int):
    """
    /top Nhï¼šæœ€è¿‘ N å°æ—¶ Top æ¦œï¼ˆåˆè®¡ï¼‰
    ä¾èµ– samples.jsonï¼ˆbot å‘¨æœŸé‡‡æ ·ï¼‰
    """
    ensure_dirs()
    if hours <= 0:
        telegram_send("ç”¨æ³•ï¼š/top 6hï¼ˆN>0ï¼‰")
        return

    # é‡‡æœ€æ–° sample
    take_sample_if_due(force=True)

    now_ts = int(time.time())
    target_ts = now_ts - hours * 3600
    base = get_sample_at_or_before(target_ts)
    if base is None:
        telegram_send(
            "âš ï¸ è¿˜æ²¡æœ‰è¶³å¤Ÿçš„é‡‡æ ·å†å²æ¥è®¡ç®—è¿™ä¸ªæ—¶é—´èŒƒå›´ã€‚\n"
            f"è¯·ä¿æŒ bot æœåŠ¡è¿è¡Œä¸€æ®µæ—¶é—´åå†è¯•ï¼š/top {hours}h"
        )
        return

    data = load_samples()
    samples = data.get("samples", [])
    if not samples:
        telegram_send("âš ï¸ é‡‡æ ·æ•°æ®ä¸ºç©ºï¼Œè¯·ç¨åå†è¯•ã€‚")
        return

    cur = samples[-1]
    deltas, reset_warnings = compute_delta_from_maps(cur.get("nodes", {}), base.get("nodes", {}))
    skipped = list(dict.fromkeys((base.get("skipped", []) or []) + (cur.get("skipped", []) or [])))

    from_dt = datetime.fromtimestamp(int(base["ts"]), TZ)
    to_dt = datetime.fromtimestamp(int(cur["ts"]), TZ)
    label = f"{from_dt.strftime('%Y-%m-%d %H:%M')} â†’ {to_dt.strftime('%Y-%m-%d %H:%M')}"
    send_top_only(label, deltas, reset_warnings, skipped=skipped)


def bootstrap_period_baselines():
    td = today_date()
    ws = start_of_week(td)
    ms = start_of_month(td)

    set_baseline_to_current(f"WEEK-{ws.strftime('%Y-%m-%d')}")
    set_baseline_to_current(f"MONTH-{ms.strftime('%Y-%m-%d')}")
    telegram_send("âœ… å·²å»ºç«‹æœ¬å‘¨ / æœ¬æœˆèµ·ç‚¹å¿«ç…§ï¼šç°åœ¨å¯ç›´æ¥ç”¨ /week /month /top week /top month")


# -------------------- Telegram å‘½ä»¤ç›‘å¬ --------------------

def get_updates(offset: int | None):
    """
    Telegram long polling åœ¨å…¬ç½‘ç¯å¢ƒä¸‹å¶å‘è¢«å¯¹ç«¯ reset æ˜¯æ­£å¸¸çš„ã€‚
    è¿™é‡Œåšï¼šç½‘ç»œé”™è¯¯è‡ªåŠ¨é‡è¯• + è½»é‡é€€é¿ï¼Œé¿å…åˆ·å±å‘Šè­¦/é¢‘ç¹é‡è¿ã€‚
    """
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/getUpdates"
    params = {"timeout": 50}
    if offset is not None:
        params["offset"] = offset

    # æœ€å¤šé‡è¯• 5 æ¬¡ï¼šæ€»ç­‰å¾… ~ (1+2+4+8+16)s + æŠ–åŠ¨
    backoff = 1.0
    last_exc = None
    for _ in range(5):
        try:
            r = HTTP_SESSION.get(url, params=params, timeout=TIMEOUT + 60)
            r.raise_for_status()
            return r.json()
        except (requests.exceptions.ConnectionError,
                requests.exceptions.ReadTimeout,
                requests.exceptions.ChunkedEncodingError) as e:
            last_exc = e
            time.sleep(backoff + random.random())
            backoff = min(backoff * 2, 20.0)
            continue

    # è¿ç»­å¤±è´¥æ‰æŠ›å‡ºï¼Œè®©å¤–å±‚é™é¢‘å‘Šè­¦æ¥ç®¡
    raise last_exc


def load_offset() -> int | None:
    try:
        with open(TG_OFFSET_PATH, "r", encoding="utf-8") as f:
            s = f.read().strip()
            return int(s) if s else None
    except FileNotFoundError:
        return None
    except Exception:
        return None


def save_offset(val: int):
    with open(TG_OFFSET_PATH, "w", encoding="utf-8") as f:
        f.write(str(val))


def parse_top_scope(text: str):
    """
    /top
    /top today|week|month
    /top 6h /top 24h /top 168h
    """
    parts = text.strip().split()
    if len(parts) == 1:
        return ("today", None)

    arg = parts[1].strip().lower()
    if arg in ("today", "t"):
        return ("today", None)
    if arg in ("week", "w"):
        return ("week", None)
    if arg in ("month", "m"):
        return ("month", None)

    m = re.fullmatch(r"(\d+)\s*h", arg)
    if m:
        return ("hours", int(m.group(1)))

    return ("unknown", None)


def listen_commands():
    ensure_dirs()
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        raise RuntimeError("TELEGRAM_BOT_TOKEN / TELEGRAM_CHAT_ID æœªè®¾ç½®")

    logging.info("Komari traffic bot starting (stat_tz=%s)", STAT_TZ)
    offset = load_offset()

    # å¯åŠ¨å…ˆé‡‡ä¸€æ¬¡æ ·
    try:
        take_sample_if_due(force=True)
    except Exception:
        pass

    while True:
        if SHUTTING_DOWN:
            logging.warning("shutdown flag set, exiting listen loop")
            return
        try:
            # å‘¨æœŸé‡‡æ ·ï¼šå“ªæ€•æ²¡äººå‘å‘½ä»¤ï¼Œä¹Ÿä¼šç§¯ç´¯ /top Nh æ‰€éœ€æ•°æ®
            try:
                take_sample_if_due(force=False)
            except Exception:
                pass

            data = get_updates(offset)
            if not data.get("ok"):
                time.sleep(3)
                continue

            for upd in data.get("result", []):
                update_id = upd.get("update_id")
                if update_id is not None:
                    offset = update_id + 1

                msg = upd.get("message") or upd.get("edited_message")
                if not msg:
                    continue

                chat = msg.get("chat", {})
                chat_id = str(chat.get("id", ""))
                if chat_id != str(TELEGRAM_CHAT_ID):
                    continue

                text = (msg.get("text") or "").strip()
                if not text.startswith("/"):
                    continue

                now = now_dt()
                td = today_date()

                if text.startswith("/today"):
                    tag = td.strftime("%Y-%m-%d")
                    run_period_report(start_of_day(td), now, tag, top_only=False)

                elif text.startswith("/week"):
                    ws = start_of_week(td)
                    tag = f"WEEK-{ws.strftime('%Y-%m-%d')}"
                    run_period_report(start_of_day(ws), now, tag, top_only=False)

                elif text.startswith("/month"):
                    ms = start_of_month(td)
                    tag = f"MONTH-{ms.strftime('%Y-%m-%d')}"
                    run_period_report(start_of_day(ms), now, tag, top_only=False)

                elif text.startswith("/top"):
                    scope, hours = parse_top_scope(text)
                    if scope == "today":
                        tag = td.strftime("%Y-%m-%d")
                        run_period_report(start_of_day(td), now, tag, top_only=True)
                    elif scope == "week":
                        ws = start_of_week(td)
                        tag = f"WEEK-{ws.strftime('%Y-%m-%d')}"
                        run_period_report(start_of_day(ws), now, tag, top_only=True)
                    elif scope == "month":
                        ms = start_of_month(td)
                        tag = f"MONTH-{ms.strftime('%Y-%m-%d')}"
                        run_period_report(start_of_day(ms), now, tag, top_only=True)
                    elif scope == "hours":
                        run_top_last_hours(int(hours or 0))
                    else:
                        telegram_send("ç”¨æ³•ï¼š/top  æˆ–  /top today|week|month  æˆ–  /top 6h")

                elif text.startswith("/archive"):
                    archive_and_prune_history()
                    telegram_send("âœ… å·²æ‰§è¡Œå†å²å½’æ¡£å‹ç¼©")

                elif text.startswith("/help") or text.startswith("/start"):
                    telegram_send(
                        "å¯ç”¨å‘½ä»¤ï¼š\n"
                        "/today  /week  /month\n"
                        "/top  (é»˜è®¤ today)\n"
                        "/top today|week|month\n"
                        "/top 6hï¼ˆä»»æ„Nhï¼‰\n"
                        "ç®¡ç†å‘˜ï¼š/archiveï¼›åˆå§‹åŒ–ï¼šè¿è¡Œ bootstrap"
                    )

            if offset is not None:
                save_offset(offset)

        except Exception as e:
            if should_alert("listen", 300):
                alert_exception("listen_loop", "listen", e)
            logging.exception("listen loop error")
            time.sleep(3)


# -------------------- main --------------------

def main():
    if len(sys.argv) < 2:
        raise RuntimeError("Usage: report_daily | report_weekly | report_monthly | listen | bootstrap | health | config-validate")

    cmd = sys.argv[1].strip().lower()

    if cmd == "report_daily":
        run_daily_send_yesterday()
        return 0
    if cmd == "report_weekly":
        run_weekly_send_last_week()
        return 0
    if cmd == "report_monthly":
        run_monthly_send_last_month()
        return 0
    if cmd == "listen":
        listen_commands()
        return 0
    if cmd == "bootstrap":
        bootstrap_period_baselines()
        return 0
    if cmd == "config-validate":
        validate_config_or_raise()
        print("OK")
        return 0
    if cmd == "health":
        validate_config_or_raise()
        run_healthcheck_or_raise()
        print("OK")
        return 0

    raise RuntimeError("Unknown command")


if __name__ == "__main__":
    setup_logging()
    signal.signal(signal.SIGTERM, _handle_sigterm)
    signal.signal(signal.SIGINT, _handle_sigterm)
    full_cmd = " ".join(sys.argv[1:]) if len(sys.argv) > 1 else "(none)"
    try:
        sys.exit(main())
    except Exception as e:
        key = f"cmd_{(sys.argv[1] if len(sys.argv) > 1 else 'none')}"
        if should_alert(key, 300):
            alert_exception("main", full_cmd, e)
        raise
